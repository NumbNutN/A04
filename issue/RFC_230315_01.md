## Request for Comment 230315:01


author: LGD

date:2023-3-15


### 主题

数据特征工程处理在效率上的改进



### 描述

分词是对网页文本快照作特征分析相当基础的一步，经过分词可以对文本在词汇层面进行词袋模型、词向量等特征模型的构建，后者往往可以作为分类任务的数据



其对一组文本集的处理如下：

```
input:["唤醒自己沉睡的财产房产","骗子竟是东方基金"]
output:["唤醒 自己 沉睡 的 财产 房产","骗子 竟是 东方 基金"]

这只是单纯利用spacy在词汇上的切割，实际上spacy还分析了句子在语义上的关联
```



### 问题

spacy模块对句子的分词模型运作效率达不到需求



一些数据展示在下面：

```
我们将train2.csv 650-800行 共计 150行文本进行分词

用时为： 19.27s

这意味着利用train2.csv 7000行数据的总耗时可达 14 min



from tool import feature_extraction_tool as fet
import spacy
import time

# 加载网页文本快照数据 train.csv
filename = "./data/train2.csv"
# 数据集范围 定义从csv文件中提取的数据的范围
data_range = range(650,800)

text_list = fet.read_csv_context(filename,data_range)

# 加载分词工具
nlp = spacy.load('zh_core_web_md')

start_time = time.time()
# 对文本进行分词
word_list = fet.split_word_arr(nlp,text_list)

end_time = time.time()


```



#### 解决途径

新拉一个数据集，这个数据集以分词后的句子作为文本数据源材料



### Comment Here

#### CJY一点想法:

+ 新拉一个数据集和对句子进行分词应该仍属于数据预处理阶段，对于整个过程效率并没有真正提升。
+ 题目并没有提供分词后的句子，并且我们最终目标要对百万级的测试集标签
+ 是否能提高spacy的效率?服务器问题?模型问题?
+ 捕捉网页文本内容时能否保留原网页分词特征?
+ 效率问题:感觉可以在我们基本完成给定任务后，在保证准确率和召回率的基
  础上针对各个步骤进行效率分析，逐步思考优化方法。

#### 回复：

+ 对数据集进行一些基本的处理有利于效率提高，因为我们记录了特征处理后数据集中间状态，而不必在每次训练模型时进行重复的工作
+ spacy分词的效率不算太低，至少小于十万级的数据集可以在服务器上花费一晚上的时间处理完，而我们可以记录这个珍贵的分词结果以避免再花无数个夜晚进行分词
+ spacy支持gpu单元加速，在我的本地平台基于AMD RADEON CPU和RTX 3060 GPU进行分词时没有充分利用到这一点，如果我们成功配置了GPU加速的必要组件，效率应该有很大改进，除此之外，（在我们力所能及的范围内）应该没有更有效的提高效率的方法
+ 捕捉网页时似乎没必要分词，成熟的分词模型已经足够好用，另一方面，中文即便在原网页本身没有天然的分词特征（如英文的空格）
